{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b84d8e-0f54-48c3-8a5f-c48161743f79",
   "metadata": {},
   "source": [
    "# Introduction to End-to-End RAPIDS Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12fb5e1-f301-484f-abaf-acfcb2c7bfc7",
   "metadata": {},
   "source": [
    "This tutorial will teach developers how to build an end-to-end workflow with cuDF, cuML, and accelerated XGBoost. You will have the chance to ingest data, conduct ETL, perform EDA, train an XGBoost model, and use SHAP to gain insights into the predictions made by the model. \n",
    "\n",
    "\n",
    "We're going to be working with data from the [CitiBike data set](https://ride.citibikenyc.com/system-data). CitiBike is a bike rental company which operates in NYC. Bikes are 'stored' at docking stations around the city, and users can rent a bike and return it to any docking station. We will use the historical information to attempt to predict the duration of a user's ride, given their starting station, as well as some other information. \n",
    "\n",
    "\n",
    "Before we begin, we're going to check what kind of GPU we have using [nvidia-smi](https://developer.nvidia.com/nvidia-system-management-interface). `nvidia-smi` has a whole range of functions described at the link. We are just going to use it to see general information about our GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf90750-1bd8-4a09-b382-4bb276dd0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888a4c0-5535-4298-8c04-f43b2cce333f",
   "metadata": {},
   "source": [
    "Here we see that we have a [16 GB card](https://www.google.com/search?q=mib+to+gb&ei=3s07YsmeELHt9AP4z5zgDg&ved=0ahUKEwjJhbPQ0N32AhWxNn0KHfgnB-wQ4dUDCA4&uact=5&oq=mib+to+gb&gs_lcp=Cgdnd3Mtd2l6EAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAELADEEMyBwgAELADEEMyBwgAELADEEMyBwgAELADEEMyCggAEOQCELADGAEyCggAEOQCELADGAEyCggAEOQCELADGAEyDwguENQCEMgDELADEEMYAjIPCC4Q1AIQyAMQsAMQQxgCSgQIQRgASgQIRhgBUABYAGCABmgBcAF4AIABAIgBAJIBAJgBAMgBEcABAdoBBggBEAEYCdoBBggCEAEYCA&sclient=gws-wiz). If we had multiple cards, we would use `dask_cudf`. This will be covered in another notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378859f-24b0-4a09-a1c3-207d4b2a388d",
   "metadata": {},
   "source": [
    "##Â Importing the data\n",
    "\n",
    "The CitiBike data is available for download directly from an s3 bucket. In the following cell, we import the data from 2014 only. You can expect the import to take around 30 seconds. \n",
    "\n",
    "\n",
    "_You can change the years and the number of years in the cell below by altering `START_YEAR` and `END_YEAR`. Just remember that the `range()` function in Python excludes the `END_YEAR`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c87a6e-246d-43bc-a302-0a3b62870dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cupy as cp\n",
    "import cudf \n",
    "\n",
    "\n",
    "START_YEAR = 2014\n",
    "END_YEAR = 2015 \n",
    "\n",
    "data_dir = './bike/'\n",
    "if not os.path.exists(data_dir):\n",
    "    print('Creating bike directory')\n",
    "    os.system('mkdir -p ./bike')\n",
    "\n",
    "url = 'https://s3.amazonaws.com/tripdata/YEARMONTH-citibike-tripdata.zip'\n",
    "    \n",
    "def get_year(START_YEAR):\n",
    "    for month in range(1,13):\n",
    "        period = str(START_YEAR)+'{:02}'.format(month)\n",
    "        url_current = url.replace('YEARMONTH', period)\n",
    "        df = cudf.read_csv(url_current, compression='zip')\n",
    "        df.to_parquet('./bike/{}/{}.parquet'.format(str(START_YEAR),period), compression='snappy', overwrite=True)\n",
    "        print(\"{}/{} saved to local storage\".format(START_YEAR, month))\n",
    "        del df\n",
    "\n",
    "for year in range(START_YEAR, END_YEAR):\n",
    "    path = './bike/{}'.format(START_YEAR)\n",
    "    if not os.path.exists(path):\n",
    "        os.system('mkdir -p {}'.format(path)+'/')\n",
    "        print('Downloading to {}'.format(path))\n",
    "        t0 = time.time()\n",
    "        get_year(START_YEAR)\n",
    "        t1 = time.time()\n",
    "        print(\"{} download time: {} seconds\".format(START_YEAR, round(t1-t0),3))\n",
    "    else:\n",
    "        print(str(year)+\" already locally available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a7c8a-3b9d-4ea0-b06e-9c766d381b42",
   "metadata": {},
   "source": [
    "Below, we read the data we saved to our local hard drive into one large cuDF DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8755d632-b7cc-4541-b2c5-e28a94740836",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['{:02}'.format(month) for month in range(1,13)]\n",
    "files = ['./bike/'+year+'/'+year+month+'.parquet' for year in os.listdir('bike') for month in months]\n",
    "df = cudf.read_parquet(files[0])\n",
    "for file in files[1:12]:\n",
    "    _df = cudf.read_parquet(file)\n",
    "    df = cudf.concat([df,_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe90e95-bc1d-44ec-a0ba-ff4a1a5c5fd3",
   "metadata": {},
   "source": [
    "Let's look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113a504-2bc6-49a8-98a2-e4ac97381e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36b29f-2967-4117-8017-ac1249e3e1a8",
   "metadata": {},
   "source": [
    "The spaces in the column names are a pain to work with, so let's change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f54c5-e098-4d75-9dd4-16725ba93b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [column.replace(' ', '_') for column in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72167fa-7e9b-4393-83a2-d0148f01f0d5",
   "metadata": {},
   "source": [
    "Let's see the data types we have from the import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3c8e1-1703-4f5f-a899-94b8dab073e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6b233-a1e5-427b-b3a8-a2afb15e1c4f",
   "metadata": {},
   "source": [
    "We'll have to change some of the types by hand. We'll only have to convert the geographic numbers and trip duration, since they are the only columns where the number represents a cardinal value, and not a category. We'll also want to convert the time stamps to time values. We'll wait to convert the time values until after we've looked at the data because time values do not play nicely with `describe()`, and we'll wait to convert the lat/long values to numbers because it will be valueable to see if they match up with the `*_station_name` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8b4a8-a6c0-4f14-b73e-129783a58d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tripduration'] = df['tripduration'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a78ad-b15f-4a85-91f3-033cc9d25364",
   "metadata": {},
   "source": [
    "Let's take a quick look at the data. We see that when `df.describe()` is given mixed types, we should tell it to include all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dd7fc-0632-4626-b9d9-ee90358b81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f2d6bd-5f2c-45c6-a082-94e12d43764d",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1078beb-1d84-4208-8f3d-cf742bb77fed",
   "metadata": {},
   "source": [
    "It looks like something weird is going on with the Station IDs - there are 332 of them, but 344 Names and 344 Long/Lat values. In 2014, there were around 8M trips. The fleet contained almost 7k bikes. The `birth_year` field seems like it has \\N as the most common year which may be missing data. We'll replace it with the average. Oddly, it also looks like `starttime` and `stoptime` are written in different formats - one with dashes and one with slashes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b157af-574d-42af-89b7-d048653f0238",
   "metadata": {},
   "source": [
    "Before we begin, let's make sure that our data is in a format that we like. Let's also drop the `start_station_id` and `end_station_id` because at best its redundant and at worst its fishy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655e7f0-0bc2-4343-aa06-bfe0e51ee0cf",
   "metadata": {},
   "source": [
    "Let's also drop observations where `tripduration` is negative. Bikes can do a lot of things, but they can't travel back in time. We will drop those observations from the dataframe. We're going to drop `bikeid` from the data set since there's no good reason to think a particular bike would have measureable impact on how long a trip lasts. While we can imagine a broken or otherwise malfunctioning bike leading to issues for the ride, we'll get around that roughly by excluding trips that are less than five minutes from the data set. We know from the link above that the `tripduration` values provided are in seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036645b-93c8-4ccf-a9e7-70a28680e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['start_station_id', 'end_station_id', 'bikeid'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99ac38-bf0f-491f-b330-8d85fda30a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tripduration'] = df['tripduration'].where(df['tripduration']>300)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae7a39-c3a4-4ec2-9c44-7e922de24d72",
   "metadata": {},
   "source": [
    "Next, we grab some things from the time fields that will be useful as features for our model. We're doing to create a variable grouping the time of day into one of six periods, the day of week, the month, and then we're going to drop those time variables. The exact second a bike was rented or returned likely has limited explanatory value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30c9bb-7787-4272-874d-e464492b7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['starttime'] = df['starttime'].astype('datetime64[s]')\n",
    "df['stoptime'] = df['stoptime'].astype('datetime64[s]')\n",
    "df['start_hour_of_the_day'] = df['starttime'].dt.hour\n",
    "df['stop_hour_of_the_day'] = df['stoptime'].dt.hour\n",
    "df['dow'] = df['starttime'].dt.dayofweek\n",
    "df['month'] = df['starttime'].dt.month\n",
    "df = df.drop(['starttime', 'stoptime'], axis=1)\n",
    "\n",
    "\n",
    "na = cp.nan\n",
    "df['birth_year'] = df['birth_year'].replace('\\\\N', na).astype('float')\n",
    "average_birth_year = df['birth_year'].mean()\n",
    "df['birth_year'] = df['birth_year'].replace(na, average_birth_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2849c3-06f2-4309-b241-0a20f5180136",
   "metadata": {},
   "source": [
    "We're going to calculate [haversine distance](https://en.wikipedia.org/wiki/Haversine_formula) for each trip. This will be a useful but imperfect measure of how far a rider traveled. After that, we'll drop lat/long values since they don't contain any information beyond `station_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5b67f-a465-41b1-9ef8-74bcc62e9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuspatial import haversine_distance\n",
    "\n",
    "df['start_station_latitude'] = df['start_station_latitude'].astype('float')\n",
    "df['start_station_longitude'] = df['start_station_longitude'].astype('float')\n",
    "df['end_station_latitude'] = df['end_station_latitude'].astype('float')\n",
    "df['end_station_longitude'] = df['end_station_longitude'].astype('float')\n",
    "\n",
    "df['trip_distance'] = haversine_distance(df['start_station_latitude'], df['start_station_longitude'], df['end_station_latitude'], df['end_station_longitude'])\n",
    "df = df.drop(['start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2b01c-71e8-4da1-854f-0c2bfa1d0d73",
   "metadata": {},
   "source": [
    "We're going to use cuML for the next bit of ETL to encode labels into numbers for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce325f5-3aef-4e66-a5dd-01ff3c3979ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "\n",
    "le = cuml.LabelEncoder()\n",
    "df['start_station_name'] = le.fit_transform(df['start_station_name'])\n",
    "df['end_station_name'] = le.fit_transform(df['end_station_name'])\n",
    "df['usertype'] = le.fit_transform(df['usertype'])\n",
    "df['gender'] = df['gender'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46b86e-7fba-46d5-a527-c61bff63b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71317b31-dcd3-48cb-a15e-dbeac1d8bd65",
   "metadata": {},
   "source": [
    "Given that we are aiming to predict the length of the ride in seconds, it seems unfair to include both the hour at which the journey starts and the hour at which the journey stops in our feature vectors - let's remove this now, and see how well we can predict trip duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a25fad-317f-4ab6-b623-a0363af717b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('stop_hour_of_the_day', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd61d9e-7631-46c8-b792-c78840ab1451",
   "metadata": {},
   "source": [
    "Now that our data is cleaned up, it's time to see how well we can predict trip duration. We'll start by making a simple XGBoost model and then we will move onto an ensemble with some other methods with cuML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d34673-1e08-4dd4-973e-947a8f311c38",
   "metadata": {},
   "source": [
    "## XGBoost Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed63a0-6efb-4316-9bcc-6bc758d45c2f",
   "metadata": {},
   "source": [
    "First, we want to split our data into train and test sets. We do this with cuML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726248f-b027-482a-9368-d0e3c4127987",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cuml.train_test_split(df, 'tripduration', train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72428e6d-36fe-4c8c-abc0-d87737e188a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1ea3d-9a23-41ee-9017-8864f053b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 9,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'subsample': 0.8,\n",
    "    'gamma': 1,\n",
    "    'disable_default_eval_metric':True, \n",
    "    'tree_method':'gpu_hist' \n",
    "}\n",
    "\n",
    "trained_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(dtrain, 'train')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0fdf0d-d20f-42c7-9d0f-2369202336db",
   "metadata": {},
   "source": [
    "We'll save the trained model, so that we can re-load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0654d51-4be7-4d6d-add6-b9f835646854",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save_model(\"xgb.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7321699-beb4-4327-8475-bb929b591970",
   "metadata": {},
   "source": [
    "Now let's see how well our fitted model looks out in the wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7c566-0ff6-4a26-ac16-5c567eefbb2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = trained_model.predict(dtest).astype('int32')\n",
    "print(\"RMSE: {}\".format(cp.sqrt(cuml.metrics.mean_squared_error(y_test.values, prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b9059-856f-4f2b-b98e-81d29ade0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "1043/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0659f7-4687-4911-a775-5e60c8b13cc5",
   "metadata": {},
   "source": [
    "So we're usually off by about 17 and a half minutes. Looks like there's some room to improve! Why not spend some time changing the model parameters and seeing if you can train a model which gives a smaller RMSE on our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7bd16-9fcc-41b6-a7d7-34783a5fc4d1",
   "metadata": {},
   "source": [
    "## Model Explainability with SHAP\n",
    "\n",
    "When using complex models, such as XGBoost, it's not always straightforward to understand the predictions made by the model. In this section we use Shapley Additive Explanation (SHAP) values to gain insight into the Machine Learning model.\n",
    "\n",
    "Computing SHAP values is a computationally expensive procedure, but we accelerate the procedure by running on NVIDIA GPUs. To save more time, we compute SHAP values on a subset of our data.\n",
    "\n",
    "Much of the code in this section is taken from this great [blog](https://medium.com/rapids-ai/gpu-accelerated-shap-values-with-xgboost-1-3-and-rapids-587fad6822) on GPU-Accelerated SHAP Values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11622a9e-8da0-4152-b05a-7208b9ebdaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_sample = xgb.DMatrix(X_test.sample(frac=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08291f46-95a6-4740-94b8-d459a37f304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trained_model.set_param({\"predictor\": \"gpu_predictor\"})\n",
    "shap_values = trained_model.predict(shap_sample, pred_contribs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a3333-f838-4c32-ad98-3c63a422b451",
   "metadata": {},
   "source": [
    "We can aggregate and visualse these SHAP values to see which of the features in our data had the most impact on the predictions made by our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d7688-7e7c-4771-95e1-65c28f3d5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance(feature_names, shap_values):\n",
    " # Get the mean absolute contribution for each feature\n",
    " aggregate = np.mean(np.abs(shap_values[:, 0:-1]), axis=0)\n",
    " # sort by magnitude\n",
    " z = [(x, y) for y, x in sorted(zip(aggregate, feature_names), reverse=True)]\n",
    " z = list(zip(*z))\n",
    " plt.bar(z[0], z[1])\n",
    " plt.xticks(rotation=90)\n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    "\n",
    "\n",
    "plot_feature_importance(X_test.columns, shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f578dc-c462-4989-8872-be17ed270199",
   "metadata": {},
   "source": [
    "This shows us that the most important features in predicting ride duration are the length of the trip, and the stations at which the trip began and ended.\n",
    "\n",
    "\n",
    "We can also use SHAP to consider the importance of interactions between features. This is more computationally expensive again, but can bring valuable insights. The following cell will take around 100 seconds to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2fb4f0-89ba-4dd1-ac95-bff88e6d9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shap_interactions = trained_model.predict(shap_sample, pred_interactions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd4e2a-d667-40c3-b8e5-0d2f08105e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_k_interactions(feature_names, shap_interactions, k):\n",
    " # Get the mean absolute contribution for each feature interaction\n",
    " aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n",
    " interactions = []\n",
    " for i in range(aggregate_interactions.shape[0]):\n",
    "     for j in range(aggregate_interactions.shape[1]):\n",
    "         if j < i:\n",
    "             interactions.append(\n",
    "             (feature_names[i] + \"-\" + feature_names[j], aggregate_interactions[i][j] * 2))\n",
    " # sort by magnitude\n",
    " interactions.sort(key=lambda x: x[1], reverse=True)\n",
    " interaction_features, interaction_values = map(tuple, zip(*interactions))\n",
    " plt.bar(interaction_features[:k], interaction_values[:k])\n",
    " plt.xticks(rotation=90)\n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    "\n",
    "\n",
    "plot_top_k_interactions(X_test.columns, shap_interactions, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70363ba3-213c-431a-ba43-b50d3c760190",
   "metadata": {},
   "source": [
    "Here we see that the interactions between the source and destination station name have the greatest impact on the predicted ride duration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0f44a-a5da-4f1b-b932-619df627d818",
   "metadata": {},
   "source": [
    "## Accelerating Inference \n",
    "\n",
    "Throughout this notebook we have run most of our computation on the GPU. In this Section, we compare the speed it takes to make predictions on a CPU vs the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39eceeb-ad1d-435d-b42f-a6a629d3f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_features = xgb.DMatrix(X_test.astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551d3bf-d3bf-4373-a3cd-4e2c2270af9a",
   "metadata": {},
   "source": [
    "### CPU\n",
    "\n",
    "We first re-load the model from file, as XGBoost caches the results of previous predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920c434-5a91-4b7d-baa1-8e042abe147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.Booster(model_file=\"xgb.model\")\n",
    "model.set_param({\"predictor\": \"cpu_predictor\"})\n",
    "predictions = model.predict(xgb_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcdce86-9436-46cf-a5e8-e5f680c007a1",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "Now we can again reload the model, and this time run the same predictions on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7cbc4-6e27-4678-99c8-a014d2a07267",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.Booster(model_file=\"xgb.model\")\n",
    "model.set_param({\"predictor\": \"gpu_predictor\"})\n",
    "predictions = model.predict(xgb_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2fc191-57c6-4c13-88ca-09539fcca314",
   "metadata": {},
   "source": [
    "So you can see that the GPU allows us to make predictions in a fraction of the time taken on CPU. This is ideal for situations requiring real-time inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b014a6-5a26-4e1a-92c1-40b84fe4bf65",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook you've seen how we can use cuML, cuDF and XGBoost to explore and clean data, compute feature vectors and train a machine learning model to predict ride duration on the CitiBike Data Set. \n",
    "\n",
    "To find out more, check out [RAPIDS.ai](http://rapids.ai)."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-rapids-22.04-py",
   "name": "common-cu110.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m91"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rapids-22.04]",
   "language": "python",
   "name": "conda-env-rapids-22.04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
