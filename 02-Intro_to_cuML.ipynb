{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ba0e05-b26e-45a8-9e72-c8797e14de05",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to cuML \n",
    "\n",
    "cuML is suite of GPU-accelerated machine learning algorithms, designed to accelerate your data science and analytical workloads. From pre-processing data through to training and evaluating models, cuML proivdes a user-friendly API and a wide range of functionality to help you get the most from your GPUs.  \n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "The following key concepts sit at the core of cuML's design, and enable you to get the most out of your data:\n",
    "\n",
    "#### 1. Where possible, match the scikit-learn API\n",
    "\n",
    "cuML estimators look and feel just like scikit-learn estimators. You initialize them with key parameters, fit them with a fit method, then call predict or transform for inference.\n",
    "\n",
    "#### 2. Accept flexible input types, return predictable output types\n",
    "\n",
    "cuML estimators can accept NumPy arrays, cuDF dataframes, cuPy arrays, 2d PyTorch tensors, and really any kind of standards-based Python array input you can throw at them.\n",
    "\n",
    "By default, outputs will mirror the data type you provided.\n",
    "\n",
    "#### 3. Be fast!\n",
    "\n",
    "On a modern GPU, these can exceed the performance of CPU-based equivalents by a factor of anything from 4x (for a medium-sized linear regression) to over 1000x (for large-scale tSNE dimensionality reduction). In many cases, performance advantages appear as the dataset grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b6218-ece1-4f4b-a5a7-21d730de0c5b",
   "metadata": {},
   "source": [
    "In this notebook we step through some of the functionality of cuML, in the context of a standard data science workflow. \n",
    "\n",
    "We begin importing the cuML module, as well as cuDF, and simulating some data to use in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a6e5ed-12f1-4e84-a992-5e4e721139f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cuml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a2b52-02b8-4f31-8484-6ee90038e956",
   "metadata": {},
   "source": [
    "In the next cell we simulate 100,000 data samples. Each sample has 70 features, and belongs to one of two distinct classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38a04c-58ca-4272-9e60-6bd3c4fd3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.datasets import make_classification, make_regression\n",
    "\n",
    "NFEATURES = 20\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=100000,\n",
    "    n_features=NFEATURES,\n",
    "    n_informative=NFEATURES,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    class_sep=0.01,\n",
    "    random_state=12\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b666d-5c3c-4ad7-8a88-6e9a2853765c",
   "metadata": {},
   "source": [
    "Let's take a look at one  sample, below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd07df4-1c28-465c-b949-301f5e1a35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0], y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247b798-b7a7-49a3-a3b6-906d9ca878d3",
   "metadata": {},
   "source": [
    "## Split data into training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c0adf-bc19-4b31-873a-447aa5c0dd54",
   "metadata": {},
   "source": [
    "We use the `train_test_split` function to divide our data into training and testing sets. \n",
    "\n",
    "We'll use the testing set later to evaluate the performance of the models we train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1e7fc-74d8-4de8-9723-404aee4268a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.model_selection import train_test_split\n",
    "\n",
    "## set train_size such that 70% of data is in the training set, 30% in the test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bbacf-2c7d-49af-96f2-4bf9417ec7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e85eca7-f0a3-4b37-92a2-149956a8551c",
   "metadata": {},
   "source": [
    "## Explore and preprocess the data\n",
    "\n",
    "Now that we have split our data into training and test sets we can begin to apply transformations. Just like scikit-learn, cuML estimators admit the _initialise_, _fit_, and _predict_ or _transform_ functionality. \n",
    "\n",
    "Let's see this in action with a the `MaxAbsScaler`. This scaler transforms each feature (column) of our data set by scaling it so that the maximum absolute value of each feature is 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0cc48-cccb-4fe9-9315-9eec95fb7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779babd-af3d-440c-90e6-560b8dcdb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the estimator\n",
    "ma_scaler = MaxAbsScaler()\n",
    "\n",
    "# fit the scaler to our training data\n",
    "ma_scaler.fit(X_train)\n",
    "\n",
    "\n",
    "# transform the testing data: \n",
    "ma_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14acaf75-40b1-48be-bccd-e0aafde4258e",
   "metadata": {},
   "source": [
    "Similarly, we can use a `RobustScaler` to transform the data so that each feature is on a similar scale. \n",
    "\n",
    "This Scaler removes the median and scales the data according to the interquantile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91ddef-950c-41e4-ac60-c10279be5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import RobustScaler\n",
    "\n",
    "# initialise the estimator\n",
    "rs = RobustScaler()\n",
    "\n",
    "# fit the estimator to the training data\n",
    "rs.fit(X_train)\n",
    "\n",
    "# transform testing data\n",
    "rs.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e4427-2e12-448b-84d2-a57f99a63b59",
   "metadata": {},
   "source": [
    "And we can inspect properties of the Scaler, such as the scale factor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9f354-2990-4c63-b7fa-8dd71a25f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188baaa-5a2c-469b-92a1-3858823ab56e",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43961a3-4af9-4d74-a08d-c2210be41808",
   "metadata": {},
   "source": [
    "When exploring our data, we often want to project the features down to 2-dimensions so that we can plot and visualise the data set, and see if we can identify patterns. \n",
    "\n",
    "We begin by using Principle Component Analysis (PCA), a linear dimensionality reduction technique. PCA requires input data to be on the same scale, so we first transform our data using the RobustScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2af7d-64a1-42bd-b115-2d4297426e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402bb41-a829-4e15-8ac0-70cb1b5b68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# initialise the estimator\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# fit the estimater to our training data\n",
    "pca.fit(rs.transform(X_train))\n",
    "\n",
    "## transform our testing data\n",
    "pca_test = pca.transform(rs.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535a853-4660-4001-a230-d13fa8307b03",
   "metadata": {},
   "source": [
    "We can examine the proportion of variance explained by the PCA and inspect the components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900d9a5-d99b-47d4-beca-cc1cecb24dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Components: {pca.components_}')\n",
    "print(f'Explained variance: {pca.explained_variance_}')\n",
    "exp_var = pca.explained_variance_ratio_\n",
    "print(f'Explained variance ratio: {exp_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c69b97-60dc-471f-a6e0-bbb1e7e5f94b",
   "metadata": {},
   "source": [
    "PCA is fast, but there are more sophisticated techniques we can use to possibly expose more structure in the data. Due to the non-linearity of these alternative dimensionality reduction techniques, they are more computationaly expensive. However, we benefit here from the acceleration provided by NVIDIA GPUs and the RAPIDS implementations. \n",
    "\n",
    "UMAP is a non-linear dimensionality reduction technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486c2b8-b122-4256-867c-65c935f21f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f328dd-d137-4573-ba43-28cf1d76ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "umap = UMAP(n_components = 2)\n",
    "umap.fit(X_train)\n",
    "umap_test = umap.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a0e6c-e367-4e26-b601-9ce865eb66ab",
   "metadata": {},
   "source": [
    "As you can see, UMAP is notably slower than PCA, but let's see if it allows us to uncover more structure in our data by plotting the projected test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f9d90-bd80-4112-be7b-f7236239d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c23bf8-6fd0-4b52-9020-a2279e91bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfering data to cpu to plot.\n",
    "umap_cpu = cp.asnumpy(umap_test)\n",
    "pca_cpu = cp.asnumpy(pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e1d30-aeb0-4d7f-a45d-815b20ac64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(umap_cpu[:,0], umap_cpu[:,1], c=cp.asnumpy(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1650e07-08b2-4bdb-b703-51639a9bb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_cpu[:,0], umap_cpu[:,1], c = cp.asnumpy(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3e3bd-1f3a-4aea-97c8-dc0cf9cccea2",
   "metadata": {},
   "source": [
    "### Training a  model \n",
    "\n",
    "Now that we've transformed our data, and have been able to identify structure in the data we can go ahead and train a model to distinguish between the two classes of data. Let's start by training a logistic regression model. \n",
    "\n",
    "Again, we follow the _initialise_, _fit_, _predict_ workflow that we used with the scalers and dimensionality reduction techniques earlier in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf302a9-2eed-48d9-bddb-dff3be0d39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise\n",
    "clr = cuml.LogisticRegression()\n",
    "\n",
    "## fit to scaled data\n",
    "clr.fit(rs.transform(X_train), y_train)\n",
    "\n",
    "## predict \n",
    "clr_preds = clr.predict(rs.transform(X_test))\n",
    "clr_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3d034b-00bf-459b-a636-bbe0b4b64d82",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "cuML provides a range of built in metrics to evaluate model performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ddc2a-812a-4570-88d3-0664ba8a8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuml.metrics.accuracy.accuracy_score(y_test, clr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a4155-5248-4114-9bf2-954bdcca711e",
   "metadata": {},
   "source": [
    "It looks like this prediction accuracy is only slightly higher than 50%. We would expect similar results by just tossing a coin to allocate classes. Let's inviestigate this further by looking at a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2e5b5-f1d0-470d-90c9-1da4535fdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuml.metrics.confusion_matrix(y_test, clr_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43bfb1-b03a-4daf-bb30-69a66de60d97",
   "metadata": {},
   "source": [
    "The confusion matrix tells us that there are many misclassifications in both the '0' and '1' classes. Let's try to train another model and see if we can get better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a46e53-abfd-42fd-a5db-eaf8e07bedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise\n",
    "ckn = cuml.neighbors.KNeighborsClassifier()\n",
    "\n",
    "# fit\n",
    "ckn.fit(rs.transform(X_train), y_train)\n",
    "\n",
    "# predict\n",
    "ckn_preds = ckn.predict(rs.transform(X_test))\n",
    "ckn_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a880f-1d4f-46f8-a9f1-7888f9457ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuml.metrics.accuracy.accuracy_score(y_test, ckn_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d43b00-6588-44ce-954a-17b52eb05f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuml.metrics.confusion_matrix(ckn_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080aaaee-cc2a-4daf-80a8-22d83df7a6c2",
   "metadata": {},
   "source": [
    "For our dataset, the k-nearest neighbour model is much better at predicting classes than the Logistic Regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6231c73-7df9-48d3-89dc-cdb328066b3d",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "To quote the wonderful scikit-learn documentation, `Pipeline` \"sequentially [applies] a list of transforms and a final estimator\" to a dataset.\n",
    "\n",
    "By collecting transformations and training into a single pipeline, we can confidently do things like cross-validation and hyper-parameter optimization without worrying about data leakage.\n",
    "\n",
    "cuML transformations and estimators are fully compatible with the scikit-learn Pipeline API.\n",
    "\n",
    "In our previous examples we used a RobustScaler followed by a k-Nearest neighbour model. Let's put those together in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207bac6-d5dd-40eb-9c82-d75ad6e8c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', cuml.preprocessing.RobustScaler()),\n",
    "    ('knn', cuml.neighbors.KNeighborsClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc06eeb-633e-42f1-bcfd-dd1a31b960aa",
   "metadata": {},
   "source": [
    "We can fit the whole pipeline in one command, and make predictions from the raw data, without having to first call the scale, then the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2427c0e-3ccc-40b0-b439-7933340c0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b88ff1-ce27-42e8-810d-9a98ab5e16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9c7a0-e1dd-46a5-9a15-02e335b78b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuml.metrics.confusion_matrix(pipe.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c20f047-efce-4d66-afa7-21fa2323513a",
   "metadata": {},
   "source": [
    "### Sidebar: comparison with scikit-learn\n",
    "\n",
    "Although we're using the scikit-learn Pipeline above, all of our data remains on the GPU thoughout the execution. Let's see how long the comparative transformations and modeling take when we run these on the CPU: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced0b7b-c926-4f39-a5bd-3fcc4d6e4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#transfer data to cpu\n",
    "cpu_X_train = pd.DataFrame(X_train)\n",
    "cpu_X_test = pd.DataFrame(X_test)\n",
    "cpu_y_train = cp.asnumpy(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d8cfb-4def-46f4-9be9-a883ee89c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import neighbors\n",
    "\n",
    "cpu_pipe = Pipeline([\n",
    "    ('scaler', sklearn.preprocessing.RobustScaler()),\n",
    "    ('knn', sklearn.neighbors.KNeighborsClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aacca5-4bd4-43ad-ba71-2d30c1bc00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cpu_pipe.fit(cpu_X_train, cpu_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ee20e-c400-43f5-b2ef-56f1eecae31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cpu_pipe.predict(cpu_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9082e3-da61-4409-bcb3-745ccb9a87dd",
   "metadata": {},
   "source": [
    "So we can run the same pipeline on CPU with no code changes needed, but it is orders of magnitude slower to do so. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfccfa-7402-4a0d-8541-cad9b35764be",
   "metadata": {},
   "source": [
    "## Explainability\n",
    "\n",
    "Model explainability is often critically important. cuML provides a GPU-accelerated SHAP Kernel Explainer and a Permutation Explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7108edd-c51f-4768-ad6d-5e0ccca8e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.explainer import KernelExplainer\n",
    "from cuml.datasets import make_classification, make_regression\n",
    "from cuml.model_selection import train_test_split\n",
    "import cuml\n",
    "Xr, yr = make_regression(\n",
    "    n_samples=102,\n",
    "    n_features=10,\n",
    "    noise=0.1,\n",
    "    random_state=42)\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    Xr,\n",
    "    yr,\n",
    "    test_size=2,\n",
    "    random_state=42)\n",
    "\n",
    "model = cuml.svm.SVR().fit(Xr_train, yr_train)\n",
    "\n",
    "cu_explainer = KernelExplainer(\n",
    "    model=model.predict,\n",
    "    data=Xr_train,\n",
    "    is_gpu_model=True)\n",
    "\n",
    "cu_shap_values = cu_explainer.shap_values(Xr_test)\n",
    "cu_shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ab982-b69a-4583-b4e8-2621bb01a484",
   "metadata": {},
   "source": [
    "## Pickling Models\n",
    "\n",
    "So far, we've only stored our models in memory. This final section demonstrates basic pickling cuML models, and pipelines, for persistence. This allows us to load these models into other environments or programs and use them to make predictions on new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7098f35-201b-458e-aec8-da97428c0000",
   "metadata": {},
   "source": [
    "We can pickle individual estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30877c-3704-41a4-a707-4fc1f13efcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e563b6-71b0-4aa9-b6be-12eea436135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"model.pkl\", \"wb\"))\n",
    "loaded_model = pickle.load(open(\"model.pkl\", \"rb\"))\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01dd79-b272-415e-9f76-ed8a2324bda9",
   "metadata": {},
   "source": [
    "We can even pickle the pipeline we made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ae6d5-c54d-48c5-ba77-7162764eaaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pipe, open(\"pipeline.pkl\", \"wb\"))\n",
    "loaded_pipeline = pickle.load(open(\"pipeline.pkl\", \"rb\"))\n",
    "\n",
    "print(loaded_pipeline.score(X_test, y_test))\n",
    "loaded_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcb75a1-e9ba-4098-ae34-9b5a958f36fd",
   "metadata": {},
   "source": [
    "We hope this notebook has shown you how you can use cuML to carry out your standard Machine Learning and analytics workflows on NVIDIA GPUs. \n",
    "\n",
    "To find out more, check out [RAPIDS.ai](http://rapids.ai) and look at the cuML [docs](https://docs.rapids.ai/api/cuml/stable/) to see the full range of the cuML functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a26b5-a824-4d2d-b97d-1e7ba617bff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-rapids-22.04-py",
   "name": "common-cu110.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m90"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rapids-22.04]",
   "language": "python",
   "name": "conda-env-rapids-22.04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
